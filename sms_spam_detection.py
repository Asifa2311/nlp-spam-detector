# -*- coding: utf-8 -*-
"""sms spam detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1enIQWy4IDF5tRCULljTDUG42QrQFANv8
"""

import pandas as pd
import numpy as np
import nltk
import re
from nltk.corpus import stopwords

df = pd.read_csv('/content/archive (9).zip')
df.head()

df = df[['v2', 'v1']]
df = df.rename(columns={'v2': 'messages', 'v1': 'label'})
df.head()

df.isnull().sum()

import nltk
import re

# Download the stopwords resource
nltk.download('stopwords')

from nltk.corpus import stopwords

# Use a different variable name
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    # convert to lowercase
    text = text.lower()
    # remove special characters
    text = re.sub(r'[^0-9a-zA-Z]', ' ', text)
    # remove extra spaces
    text = re.sub(r'\s+', ' ', text)
    # remove stopwords
    text = " ".join(word for word in text.split() if word not in STOPWORDS)
    return text

df['clean_text'] = df['messages'].apply(clean_text)
df.head()

X = df['clean_text']
y = df['label']

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Fit and transform the training data
X_train_vec = vectorizer.fit_transform(X_train)

# Transform the testing data
X_test_vec = vectorizer.transform(X_test)

# Train a logistic regression model
model = LogisticRegression()
model.fit(X_train_vec, y_train)

# Make predictions on the testing data
y_pred = model.predict(X_test_vec)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# You can also print a classification report for more detailed metrics
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier

# Train a Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators
rf_model.fit(X_train_vec, y_train)

# Make predictions on the testing data
y_pred_rf = rf_model.predict(X_test_vec)

# Calculate the accuracy of the Random Forest model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print("Random Forest Accuracy:", accuracy_rf)

# Print a classification report for the Random Forest model
print(classification_report(y_test, y_pred_rf))

!pip install matplotlib # install matplotlib using pip

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt # import matplotlib.pyplot as plt

# Assuming X_train_vec is your TF-IDF vectorized training data

# Find the optimal number of clusters using silhouette score
silhouette_scores = []
for n_clusters in range(2, 11):  # Try clusters from 2 to 10
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(X_train_vec)
    silhouette_avg = silhouette_score(X_train_vec, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Plot the silhouette scores to find the elbow point (optimal number of clusters)
plt.plot(range(2, 11), silhouette_scores)
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.title("Silhouette Analysis for KMeans Clustering")
plt.show()

# Choose the number of clusters based on the plot (e.g., the highest silhouette score)
optimal_n_clusters = 2  # Replace with the optimal number you found

# Apply KMeans with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=42)
cluster_labels = kmeans.fit_predict(X_train_vec)

# Create a dictionary to store the top words for each cluster
cluster_top_words = {}

for cluster_num in range(optimal_n_clusters):
    # Get the indices of data points belonging to the current cluster
    cluster_indices = np.where(cluster_labels == cluster_num)[0]

    # Get the TF-IDF vectors for the data points in the current cluster
    cluster_tfidf = X_train_vec[cluster_indices]

    # Calculate the average TF-IDF values for each word in the cluster
    avg_tfidf = cluster_tfidf.mean(axis=0)

    # Get the indices of the top TF-IDF values (most frequent words)
    # Use the A attribute to convert the matrix to an array
    top_word_indices = np.argsort(avg_tfidf.A[0])[::-1][:10]  # Get top 10 words

    # Get the corresponding words from the vectorizer's vocabulary
    top_words = [vectorizer.get_feature_names_out()[i] for i in top_word_indices]

    # Store the top words for the current cluster
    cluster_top_words[cluster_num] = top_words

# Print the top words for each cluster
for cluster_num, top_words in cluster_top_words.items():
    print(f"Cluster {cluster_num}: {', '.join(top_words)}")

from sklearn.svm import SVC

# Train an SVM model
svm_model = SVC(kernel='linear', random_state=42)  # You can choose different kernels (e.g., 'rbf', 'poly')
svm_model.fit(X_train_vec, y_train)

# Make predictions on the testing data
y_pred_svm = svm_model.predict(X_test_vec)

# Calculate the accuracy of the SVM model
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print("SVM Accuracy:", accuracy_svm)

# Print a classification report for the SVM model
print(classification_report(y_test, y_pred_svm))

from sklearn.naive_bayes import MultinomialNB

# Train a Naive Bayes model
nb_model = MultinomialNB()
nb_model.fit(X_train_vec, y_train)

# Make predictions on the testing data
y_pred_nb = nb_model.predict(X_test_vec)

# Calculate the accuracy of the Naive Bayes model
accuracy_nb = accuracy_score(y_test, y_pred_nb)
print("Naive Bayes Accuracy:", accuracy_nb)

# Print a classification report for the Naive Bayes model
print(classification_report(y_test, y_pred_nb))

import matplotlib.pyplot as plt

# Assuming you have your data in a DataFrame called 'df' and you want to visualize the 'label' distribution
label_counts = df['label'].value_counts()

# Create a bar chart
plt.figure(figsize=(8, 6))
plt.bar(label_counts.index, label_counts.values)
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Distribution of Labels')
plt.show()

import pandas as pd
from sklearn.metrics import accuracy_score

# Example accuracy values - replace with your actual accuracy scores
accuracy = 0.85
accuracy_rf = 0.92
accuracy_svm = 0.88
accuracy_nb = 0.80

data = {'Model': ['Logistic Regression', 'Random Forest', 'SVM', 'Naive Bayes'],
        'Accuracy': [accuracy, accuracy_rf, accuracy_svm, accuracy_nb]}
accuracy_df = pd.DataFrame(data)
print(accuracy_df)

data = {
    'Model': ['Logistic Regression', 'Random Forest', 'SVM', 'Naive Bayes'],
    'Precision': [0.95, 0.99, 0.98, 0.92],
    'Recall': [0.84, 0.93, 0.94, 0.83],
    'F1-score': [0.89, 0.95, 0.96, 0.86],
    'Support': [1115, 1115, 1115, 1115]
}

comparison_df = pd.DataFrame(data)
print(comparison_df)

import matplotlib.pyplot as plt

# Create a figure and axes
fig, ax = plt.subplots(figsize=(10, 6))

# Define colors for each metric
colors = ['blue', 'green', 'red', 'orange']

# Plot each metric as a line with different colors
for i, metric in enumerate(['Precision', 'Recall', 'F1-score']):
  ax.plot(data['Model'], data[metric], marker='o', label=metric, color=colors[i])

# Customize the plot
ax.set_xlabel('Model')
ax.set_ylabel('Score')
ax.set_title('Model Performance Comparison')
ax.legend()
ax.grid(True)

# Rotate x-axis labels if needed
plt.xticks( ha='right')

# Show the plot
plt.tight_layout()
plt.show()